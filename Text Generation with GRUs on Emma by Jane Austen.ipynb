{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "possible-quality",
   "metadata": {},
   "source": [
    "# Text Generation with GRUs on Emma by Jane Austen\n",
    "\n",
    "An example of text generation with gated recurrent units. The model is trained on data from Jane Austen's Emma. The model attempts to predict the following character based on a sequence lenght of 50 characters. After the model is trained, sample text predictions are shown at the bottom of the notebook with a handful of starting phrases to get the predictions going.\n",
    "\n",
    "This code was adapted from the following tutorial on `tensorflow.org`: https://www.tensorflow.org/text/tutorials/text_generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "buried-wesley",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from tensorflow import keras\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "criminal-vegetation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "880022\n"
     ]
    }
   ],
   "source": [
    "# READ IN ALL LINES FROM EMMA\n",
    "file = 'GPT/emma.txt'\n",
    "\n",
    "with open(file) as data:\n",
    "    raw_dataset = [x for x in data.readlines()]\n",
    "\n",
    "# Create clean version of emma\n",
    "no_new_lines = [x.replace('\\n', '').strip() for x in raw_dataset]\n",
    "while(\"\" in no_new_lines) : \n",
    "    no_new_lines.remove(\"\")\n",
    "\n",
    "dataset = no_new_lines[14:13696]\n",
    "\n",
    "text = \" \".join(dataset)\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "expected-induction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of unique characters in the text: 76\n"
     ]
    }
   ],
   "source": [
    "vocab = sorted(set(text))\n",
    "print(f\"# of unique characters in the text: {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "central-shipping",
   "metadata": {},
   "source": [
    "## Vectorize the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "critical-final",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encodes characters to IDs\n",
    "ids_from_chars = preprocessing.StringLookup(vocabulary=list(vocab), mask_token=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "negative-opera",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reverse encodes the IDs back into characters\n",
    "chars_from_ids = tf.keras.layers.experimental.preprocessing.StringLookup(vocabulary=ids_from_chars.get_vocabulary(),\n",
    "                                                                         invert=True, mask_token=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "atmospheric-stewart",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to turn tensors of IDs back into text\n",
    "def text_from_ids(ids):\n",
    "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electric-macro",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "afraid-oxide",
   "metadata": {},
   "source": [
    "## Creating Examples and Targets texts for GRUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "distinguished-deadline",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "positive-coupon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(880022,), dtype=int64, numpy=array([25, 33, 33, ..., 63, 62,  9])>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a tensor of all the IDs in a numpy array equal to the length of the text\n",
    "all_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "entertaining-reservation",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abstract-logan",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 50\n",
    "examples_per_epoch = len(text)//(seq_length+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "approximate-offset",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "indoor-clearing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'EMMA By Jane Austen VOLUME I CHAPTER I Emma Woodhou'\n",
      "b'se, handsome, clever, and rich, with a comfortable '\n",
      "b'home and happy disposition, seemed to unite some of'\n",
      "b' the best blessings of existence; and had lived nea'\n",
      "b'rly twenty-one years in the world with very little '\n"
     ]
    }
   ],
   "source": [
    "for seq in sequences.take(5):\n",
    "    print(text_from_ids(seq).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "textile-gossip",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes each individual sequence and breaks breaks it into the first 1-100 char as input\n",
    "# and 2-101 characters as target.\n",
    "def split_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "agreed-citation",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "conventional-receiver",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : b'EMMA By Jane Austen VOLUME I CHAPTER I Emma Woodho'\n",
      "Target: b'MMA By Jane Austen VOLUME I CHAPTER I Emma Woodhou'\n",
      "Input : b'se, handsome, clever, and rich, with a comfortable'\n",
      "Target: b'e, handsome, clever, and rich, with a comfortable '\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in dataset.take(2):\n",
    "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
    "    print(\"Target:\", text_from_ids(target_example).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "negative-philadelphia",
   "metadata": {},
   "source": [
    "### Prep dataset for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "precious-charleston",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ((64, 50), (64, 50)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = (\n",
    "    dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuffed-milwaukee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "needed-publicity",
   "metadata": {},
   "source": [
    "# Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "unnecessary-coordination",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "interim-volleyball",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "    super().__init__(self)\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True)\n",
    "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "  def call(self, inputs, states=None, return_state=False, training=False):\n",
    "    x = inputs\n",
    "    x = self.embedding(x, training=training)\n",
    "    if states is None:\n",
    "      states = self.gru.get_initial_state(x)\n",
    "    x, states = self.gru(x, initial_state=states, training=training)\n",
    "    x = self.dense(x, training=training)\n",
    "\n",
    "    if return_state:\n",
    "      return x, states\n",
    "    else:\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cubic-toner",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel(\n",
    "    # Be sure the vocabulary size matches the `StringLookup` layers.\n",
    "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "immediate-dublin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 50, 77) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "nasty-petersburg",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  19712     \n",
      "_________________________________________________________________\n",
      "gru (GRU)                    multiple                  1182720   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  39501     \n",
      "=================================================================\n",
      "Total params: 1,241,933\n",
      "Trainable params: 1,241,933\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blond-cheat",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "killing-vault",
   "metadata": {},
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "second-stockholm",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "absolute-gravity",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "minimal-consultation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "completed-dutch",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "threatened-teacher",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "269/269 [==============================] - 78s 281ms/step - loss: 2.8223\n",
      "Epoch 2/10\n",
      "269/269 [==============================] - 74s 271ms/step - loss: 1.7601\n",
      "Epoch 3/10\n",
      "269/269 [==============================] - 78s 287ms/step - loss: 1.4458\n",
      "Epoch 4/10\n",
      "269/269 [==============================] - 76s 281ms/step - loss: 1.3122\n",
      "Epoch 5/10\n",
      "269/269 [==============================] - 76s 278ms/step - loss: 1.2391\n",
      "Epoch 6/10\n",
      "269/269 [==============================] - 81s 298ms/step - loss: 1.1917\n",
      "Epoch 7/10\n",
      "269/269 [==============================] - 79s 289ms/step - loss: 1.1564\n",
      "Epoch 8/10\n",
      "269/269 [==============================] - 78s 286ms/step - loss: 1.1293\n",
      "Epoch 9/10\n",
      "269/269 [==============================] - 78s 288ms/step - loss: 1.0992\n",
      "Epoch 10/10\n",
      "269/269 [==============================] - 77s 284ms/step - loss: 1.0774\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dressed-objective",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "inappropriate-pollution",
   "metadata": {},
   "source": [
    "# Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "coordinate-stevens",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneStep(tf.keras.Model):\n",
    "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=0.6):\n",
    "    super().__init__()\n",
    "    self.temperature = temperature\n",
    "    self.model = model\n",
    "    self.chars_from_ids = chars_from_ids\n",
    "    self.ids_from_chars = ids_from_chars\n",
    "\n",
    "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
    "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
    "    sparse_mask = tf.SparseTensor(\n",
    "        # Put a -inf at each bad index.\n",
    "        values=[-float('inf')]*len(skip_ids),\n",
    "        indices=skip_ids,\n",
    "        # Match the shape to the vocabulary\n",
    "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
    "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "  @tf.function\n",
    "  def generate_one_step(self, inputs, states=None):\n",
    "    # Convert strings to token IDs.\n",
    "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "\n",
    "    # Run the model.\n",
    "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
    "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
    "                                          return_state=True)\n",
    "    # Only use the last prediction.\n",
    "    predicted_logits = predicted_logits[:, -1, :]\n",
    "    predicted_logits = predicted_logits/self.temperature\n",
    "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
    "    predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "    # Sample the output logits to generate token IDs.\n",
    "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "    # Convert from token ids to characters\n",
    "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "\n",
    "    # Return the characters and model state.\n",
    "    return predicted_chars, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "molecular-frame",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "extraordinary-public",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emma. The heat what was with the same personal chance of the whole. You must take the sort of speaking o\n",
      "\n",
      "The boy. Emma was soon concerned, and after all the screeg perhaps are quite as if he would not be so very \n",
      "\n",
      "Long ago. He was only to say that if she would never have thought of the ladies respect, and was not been a \n",
      "\n",
      "Once upon a time there not to be as much mischief of the engagement was by the last to the party from her to another \n",
      "\n",
      "She knewy more mistaken--and a few more than she found as to the party, however, she looked of any the Aston\n",
      "\n",
      "\n",
      "________________________________________________________________________________\n",
      "Run time: 0.13445401191711426\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['Emma', 'The boy', 'Long ago', 'Once upon a time ', 'She knew'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(100):\n",
    "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "\n",
    "for result_ in result:\n",
    "    print(result_.numpy().decode('utf-8'))\n",
    "    print()\n",
    "\n",
    "print('\\n' + '_'*80 + '\\nRun time:', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustainable-prize",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "israeli-magazine",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
